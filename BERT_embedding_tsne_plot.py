# -*- coding: utf-8 -*-
"""AES_BERT_Emmbed.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1azFJi3BuIvaMyx5OD8jk_SZ5FIgZn3rV
"""

# !pip install transformers

# Mount Google Drive to access the input file and save outputs
from google.colab import drive
drive.mount('/content/drive')

# Import libraries
import pandas as pd
import numpy as np
import torch
from transformers import BertTokenizer, BertModel
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# Load data
file_path = '/content/drive/My Drive/Colab Notebooks/combined_LLMs_file.xlsx'
data = pd.read_excel(file_path)
essays = data['Essay'].tolist()
sources = data['Respondent'].tolist()  # column for labeling in t-SNE
prompt_types = data['Prompt Type'].tolist()

# Combine Prompt Type with Essay content for embeddings
essays_with_prompt = [f"{prompt_type} {essay}" for prompt_type, essay in zip(prompt_types, essays)]

# Setup BERT Model and Tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased')
model.eval()  # Set the model to evaluation mode

# Define function to tokenize essays
def tokenize_essays(essays, max_length=512):
    return tokenizer(essays, padding=True, truncation=True, max_length=max_length, return_tensors="pt")

# Define function to generate embeddings from tokenized essays
def generate_embeddings(tokenized_essays):
    with torch.no_grad():  # disable gradient computation to save memory and computations
        outputs = model(**tokenized_essays)
        embeddings = outputs.pooler_output  # extract pooled output as document-level embeddings
    return embeddings

# Define function to process essays in batches to manage memory efficiently
def batch_embeddings(essays, batch_size=10):
    batched_embeddings = []
    for i in range(0, len(essays), batch_size):
        batch = essays[i:i + batch_size]
        tokenized = tokenize_essays(batch)
        embeddings = generate_embeddings(tokenized)
        batched_embeddings.append(embeddings)
    return torch.cat(batched_embeddings, dim=0)

# Process essays to generate embeddings
essay_embeddings = batch_embeddings(essays_with_prompt, batch_size=10)

# Save the embeddings
save_path = '/content/drive/My Drive/Colab Notebooks/LLMs_essays_embeddings_with_prompt.pt'
torch.save(essay_embeddings, save_path)

# Load the embeddings from the file for further use
loaded_embeddings = torch.load(save_path)


### VISUALIZATION ###
# Mount Google Drive to access the input file and save outputs
from google.colab import drive
drive.mount('/content/drive')

#import again for running multiple times
import os
import torch
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import numpy as np
import plotly.express as px
import pandas as pd

# Load the original data
file_path = '/content/drive/My Drive/Colab Notebooks/combined_LLMs_file.xlsx'
data = pd.read_excel(file_path)

# Load the embeddings
path_to_embeddings = '/content/drive/My Drive/Colab Notebooks/LLMs_essays_embeddings_with_prompt.pt'
essay_embeddings = torch.load(path_to_embeddings)

# Get Prompt Type labels
prompt_types = data['Prompt Type'].tolist()

# Apply t-SNE
def apply_tsne(embeddings, initial_perplexity=40):
    if embeddings is not None and embeddings.size(0) > 1:  # Ensure there are more than one sample
        perplexity = min(embeddings.size(0) - 1, initial_perplexity)
        tsne_model = TSNE(n_components=2, perplexity=perplexity, random_state=123)
        return tsne_model.fit_transform(embeddings.detach().cpu().numpy())
    else:
        return None

# Apply t-SNE to all embeddings
reduced_embeddings = apply_tsne(essay_embeddings)  # Apply to all embeddings

# Define custom markers and colors for each Prompt Type
custom_styles = {
    'NARR': {'color': 'orange', 'marker': 'v'},
    'LETT': {'color': 'blue', 'marker': 'v'},
    'COMM': {'color': 'green', 'marker': 'v'},
    'RESP': {'color': 'red', 'marker': 'v'},
    'SUGG': {'color': 'purple', 'marker': 'v'},
    'ARG': {'color': 'black', 'marker': 'v'},
}

# Visualization function
def plot_tsne_by_prompt_type(reduced_embeddings, prompt_types, title):
    if reduced_embeddings is not None:
        plt.figure(figsize=(14, 10))
        plt.title(title)
        plt.xlabel('t-SNE Component 1')
        plt.ylabel('t-SNE Component 2')

        # Get unique prompt types
        unique_prompt_types = list(set(prompt_types))

        # Plot each prompt type with custom styles
        for prompt_type in unique_prompt_types:
            indices = [i for i, pt in enumerate(prompt_types) if pt == prompt_type]
            style = custom_styles.get(prompt_type, {'color': 'black', 'marker': 'o'})  # Default style
            plt.scatter(reduced_embeddings[indices, 0], reduced_embeddings[indices, 1],
                        c=style['color'], marker=style['marker'], label=prompt_type, alpha=0.8, s=10)

        # Add legend to the plot
        plt.legend(title="Prompt Type")
    else:
        plt.text(0.5, 0.5, 'Not enough data for visualization', horizontalalignment='center')

# Plot all embeddings with t-SNE by Prompt Type
plot_tsne_by_prompt_type(reduced_embeddings, prompt_types, 't-SNE Visualization of LLMs Essays by Prompt Type')

# Ensure the directory exists
save_dir = '/Users/koketch/Desktop/t-SNE/'
os.makedirs(save_dir, exist_ok=True)

# Save the plot
plt.savefig(os.path.join(save_dir, 'tsne.png'), dpi=4000)
plt.show()


###Human & LLMs###
# # Mount Google Drive to access the input file and save outputs
# from google.colab import drive
# drive.mount('/content/drive')

# import torch
# from sklearn.manifold import TSNE
# import matplotlib.pyplot as plt
# import numpy as np

# # Load the embeddings
# path_to_embeddings = '/content/drive/My Drive/Colab Notebooks/Essay_embeddings.pt'
# essay_embeddings = torch.load(path_to_embeddings)

# # Define the full list of sources and their counts
# # source_labels = ['Human','GPT-3.5', 'GPT-4', 'GPT-4o', 'Llama-2', 'Llama-3', 'Llama-3.1']
# # source_counts = [15445, 1537, 1487, 1527, 1537, 1537, 1537]
# source_labels = ['GPT-3.5', 'GPT-4', 'GPT-4o', 'Llama-2', 'Llama-3', 'Llama-3.1']
# source_counts = [1537, 1487, 1527, 1537, 1537, 1537]

# # Generate sources list based on actual counts
# sources = []
# for label, count in zip(source_labels, source_counts):
#     sources.extend([label] * count)

# # Define custom colors and markers for each source
# custom_styles = {
#     #'Human': {'color': 'orange', 'marker': '^'},
#     'GPT-3.5': {'color': 'blue', 'marker': '*'},
#     'GPT-4': {'color': 'darkred', 'marker': '^'},
#     'GPT-4o': {'color': 'green', 'marker': '^'},
#     'Llama-2': {'color': 'blue', 'marker': '^'},
#     'Llama-3': {'color': 'darkred', 'marker': '*'},
#     'Llama-3.1': {'color': 'green', 'marker': '*'}
# }

# # Apply t-SNE
# def apply_tsne(embeddings, initial_perplexity=40):
#     if embeddings is not None and embeddings.size(0) > 1:  # Ensure there are more than one sample
#         perplexity = min(embeddings.size(0) - 1, initial_perplexity)
#         tsne_model = TSNE(n_components=2, perplexity=perplexity, random_state=123)
#         return tsne_model.fit_transform(embeddings.detach().cpu().numpy())
#     else:
#         return None

# # Apply t-SNE to all embeddings
# reduced_embeddings = apply_tsne(essay_embeddings)

# # Visualization function
# def plot_tsne(reduced_embeddings, sources, title):
#     if reduced_embeddings is not None:
#         plt.figure(figsize=(14, 10))
#         plt.title(title)
#         plt.xlabel('t-SNE Component 1')
#         plt.ylabel('t-SNE Component 2')

#         # Plot each source with its custom style
#         for source, style in custom_styles.items():
#             plot_indices = [i for i, src in enumerate(sources[:len(reduced_embeddings)]) if src == source]
#             plt.scatter(reduced_embeddings[plot_indices, 0], reduced_embeddings[plot_indices, 1],
#                         c=style['color'], marker=style['marker'], label=source, alpha=0.8, s=10)  # Marker size s=10
#         # Add legend to the plot
#         plt.legend()
#     else:
#         plt.text(0.5, 0.5, 'Not enough data for visualization', horizontalalignment='center')
#     # Save the plot
#     plt.savefig('/Users/koketch/Desktop/t-SNE/tsne.png', dpi=1000)
#     plt.show()
# # Plot all embeddings with t-SNE
# plot_tsne(reduced_embeddings, sources, 't-SNE Visualization of All Essays')