{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\"\"\"\n",
    "# LLM Essay Scoring Evaluation (Zero-Shot & Few-Shot)\n",
    "\n",
    "This notebook evaluates the scoring performance of the LLMs across the two datasets (`FCE`, `ASAP`) and two learning settings (`0-Shot`, `1-Shot`). \n",
    "\n",
    "## Instructions:\n",
    "- Choose `dataset_name`: `\"FCE\"` or `\"ASAP\"`\n",
    "- Choose `setting`: `\"0-Shot\"` or `\"1-Shot\"`\n",
    "\n",
    "The script computes:\n",
    "- MSE, MAE\n",
    "- Micro and Macro Quadratic Weighted Kappa (QWK)\n",
    "- Pearson and Spearman Correlations\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Configuration\n",
    "dataset_name = \"ASAP\"       # \"FCE\" or \"ASAP\"\n",
    "setting = \"0-Shot\"          # \"0-Shot\" or \"1-Shot\"\n",
    "\n",
    "file_path = f\"./Data/{dataset_name}.xlsx\"\n",
    "output_prefix = f\"/Users/koketch/Desktop/{setting}-{dataset_name}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mport Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, cohen_kappa_score\n",
    "from scipy.stats import pearsonr, spearmanr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt_ID</th>\n",
       "      <th>Test_Bed</th>\n",
       "      <th>Prompt_Type</th>\n",
       "      <th>Student_Essay</th>\n",
       "      <th>Human</th>\n",
       "      <th>0-Shot Rubric</th>\n",
       "      <th>GPT4_0-Shot</th>\n",
       "      <th>GPT4o_0-Shot</th>\n",
       "      <th>GPT3.5_0-Shot</th>\n",
       "      <th>Llama2_0-Shot</th>\n",
       "      <th>...</th>\n",
       "      <th>GPT4_1-Shot</th>\n",
       "      <th>GPT4o_1-Shot</th>\n",
       "      <th>GPT3.5_1-Shot</th>\n",
       "      <th>Llama3_1-Shot</th>\n",
       "      <th>Llama2_1-Shot</th>\n",
       "      <th>Llama3.1_1-Shot</th>\n",
       "      <th>Deepseek-R1_1-Shot</th>\n",
       "      <th>Qwen2.5_1-Shot</th>\n",
       "      <th>Llama3-8B_1-Shot</th>\n",
       "      <th>Prometheus-13b_1-Shot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ASAP</td>\n",
       "      <td>ARG</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Instruction\\nAs a virtual assessor, your respo...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ASAP</td>\n",
       "      <td>ARG</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Instruction\\nAs a virtual assessor, your respo...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>ASAP</td>\n",
       "      <td>ARG</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>Instruction\\nAs a virtual assessor, your respo...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>ASAP</td>\n",
       "      <td>ARG</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Instruction\\nAs a virtual assessor, your respo...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>ASAP</td>\n",
       "      <td>ARG</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Instruction\\nAs a virtual assessor, your respo...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prompt_ID Test_Bed Prompt_Type  \\\n",
       "0          1     ASAP         ARG   \n",
       "1          1     ASAP         ARG   \n",
       "2          1     ASAP         ARG   \n",
       "3          1     ASAP         ARG   \n",
       "4          1     ASAP         ARG   \n",
       "\n",
       "                                       Student_Essay  Human  \\\n",
       "0  Dear local newspaper, I think effects computer...    4.0   \n",
       "1  Dear @CAPS1 @CAPS2, I believe that using compu...    4.5   \n",
       "2  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...    3.5   \n",
       "3  Dear Local Newspaper, @CAPS1 I have found that...    5.0   \n",
       "4  Dear @LOCATION1, I know having computers has a...    4.0   \n",
       "\n",
       "                                       0-Shot Rubric  GPT4_0-Shot  \\\n",
       "0  Instruction\\nAs a virtual assessor, your respo...            2   \n",
       "1  Instruction\\nAs a virtual assessor, your respo...            2   \n",
       "2  Instruction\\nAs a virtual assessor, your respo...            3   \n",
       "3  Instruction\\nAs a virtual assessor, your respo...            3   \n",
       "4  Instruction\\nAs a virtual assessor, your respo...            3   \n",
       "\n",
       "   GPT4o_0-Shot  GPT3.5_0-Shot  Llama2_0-Shot  ...  GPT4_1-Shot  GPT4o_1-Shot  \\\n",
       "0             2              3              6  ...          3.0           2.0   \n",
       "1             2              3              4  ...          4.0           4.0   \n",
       "2             2              2              6  ...          3.0           3.0   \n",
       "3             2              3              4  ...          5.0           4.0   \n",
       "4             3              3              4  ...          4.0           4.0   \n",
       "\n",
       "   GPT3.5_1-Shot  Llama3_1-Shot  Llama2_1-Shot  Llama3.1_1-Shot  \\\n",
       "0            4.5            4.0            4.5              4.0   \n",
       "1            4.0            4.0            4.5              4.0   \n",
       "2            3.5            3.5            4.5              4.0   \n",
       "3            5.5            4.0            4.5              5.5   \n",
       "4            4.0            4.0            4.5              4.0   \n",
       "\n",
       "   Deepseek-R1_1-Shot Qwen2.5_1-Shot  Llama3-8B_1-Shot  Prometheus-13b_1-Shot  \n",
       "0                 4.0            4.0               2.0                    3.0  \n",
       "1                 4.0            4.0               3.5                    5.0  \n",
       "2                 4.0            3.0               4.5                    2.0  \n",
       "3                 4.0            4.5               4.5                    4.0  \n",
       "4                 4.0            4.5               4.0                    5.0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Data\n",
    "combined_df = pd.read_excel(file_path)\n",
    "combined_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model names \n",
    "base_models = [\n",
    "    'GPT3.5', 'GPT4', 'GPT4o', 'Llama2',\n",
    "    'Llama3', 'Llama3.1', 'Deepseek-R1',\n",
    "    'Qwen2.5', 'Llama3-8B', 'Prometheus-13b'\n",
    "]\n",
    "model_names = [f\"{m}_{setting}\" for m in base_models]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize containers\n",
    "results = []\n",
    "per_prompt_qwk_all = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric computation\n",
    "for model in model_names:\n",
    "    filtered_df = combined_df[['Prompt_ID', 'Human', model]].dropna()\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        print(f\"Skipping {model} due to insufficient data.\")\n",
    "        continue\n",
    "\n",
    "    y_true_scaled_list = []\n",
    "    y_pred_scaled_list = []\n",
    "    y_true_original_list = []\n",
    "    y_pred_original_list = []\n",
    "\n",
    "    grouped = filtered_df.groupby('Prompt_ID')\n",
    "\n",
    "    for prompt_id, group in grouped:\n",
    "        scaler = MinMaxScaler()\n",
    "\n",
    "        y_true = group['Human'].values\n",
    "        y_true_scaled = scaler.fit_transform(y_true.reshape(-1, 1)).flatten()\n",
    "        y_true_scaled_list.extend(y_true_scaled)\n",
    "\n",
    "        y_pred = group[model].values\n",
    "        y_pred_scaled = scaler.transform(y_pred.reshape(-1, 1)).flatten()\n",
    "        y_pred_scaled_list.extend(y_pred_scaled)\n",
    "\n",
    "        y_true_original_list.extend(y_true)\n",
    "        y_pred_original_list.extend(y_pred)\n",
    "\n",
    "        y_true_int = group['Human'].astype(int)\n",
    "        y_pred_int = group[model].astype(int)\n",
    "\n",
    "        if len(np.unique(y_true_int)) > 1 and len(np.unique(y_pred_int)) > 1:\n",
    "            qwk_prompt = cohen_kappa_score(y_true_int, y_pred_int, weights='quadratic')\n",
    "            per_prompt_qwk_all.append({\n",
    "                'Model': model.replace(f'_{setting}', ''),\n",
    "                'Prompt_ID': prompt_id,\n",
    "                'QWK': round(qwk_prompt, 4),\n",
    "                'Setting': setting\n",
    "            })\n",
    "\n",
    "    y_true_scaled = np.array(y_true_scaled_list)\n",
    "    y_pred_scaled = np.array(y_pred_scaled_list)\n",
    "    y_true_original = np.array(y_true_original_list)\n",
    "    y_pred_original = np.array(y_pred_original_list)\n",
    "\n",
    "    micro_qwk = round(cohen_kappa_score(y_true_original.astype(int), y_pred_original.astype(int), weights='quadratic'), 4)\n",
    "    model_qwks = [entry['QWK'] for entry in per_prompt_qwk_all if entry['Model'] == model.replace(f'_{setting}', '')]\n",
    "    macro_qwk = round(np.mean(model_qwks), 4) if model_qwks else np.nan\n",
    "\n",
    "    mse = round(mean_squared_error(y_true_scaled, y_pred_scaled), 4)\n",
    "    mae = round(mean_absolute_error(y_true_scaled, y_pred_scaled), 4)\n",
    "    pcc, _ = pearsonr(y_true_scaled, y_pred_scaled)\n",
    "    src, _ = spearmanr(y_true_scaled, y_pred_scaled)\n",
    "\n",
    "    results.append({\n",
    "        'Model': model.replace(f'_{setting}', ''),\n",
    "        'MSE': mse,\n",
    "        'MAE': mae,\n",
    "        'Micro QWK': micro_qwk,\n",
    "        'Macro QWK': macro_qwk,\n",
    "        'PCC': round(pcc, 4),\n",
    "        'SRC': round(src, 4),\n",
    "        'Setting': setting\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrames\n",
    "results_df = pd.DataFrame(results)\n",
    "per_prompt_qwk_df = pd.DataFrame(per_prompt_qwk_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Metrics:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>Micro QWK</th>\n",
       "      <th>Macro QWK</th>\n",
       "      <th>PCC</th>\n",
       "      <th>SRC</th>\n",
       "      <th>Setting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT3.5</td>\n",
       "      <td>0.2331</td>\n",
       "      <td>0.3957</td>\n",
       "      <td>0.2057</td>\n",
       "      <td>0.1271</td>\n",
       "      <td>0.1780</td>\n",
       "      <td>0.1336</td>\n",
       "      <td>0-Shot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT4</td>\n",
       "      <td>0.3083</td>\n",
       "      <td>0.4517</td>\n",
       "      <td>0.8889</td>\n",
       "      <td>0.2699</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.4441</td>\n",
       "      <td>0-Shot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GPT4o</td>\n",
       "      <td>0.2539</td>\n",
       "      <td>0.4228</td>\n",
       "      <td>0.1924</td>\n",
       "      <td>0.1431</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.2091</td>\n",
       "      <td>0-Shot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama2</td>\n",
       "      <td>1.2315</td>\n",
       "      <td>0.9558</td>\n",
       "      <td>0.1753</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>-0.0338</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0-Shot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Llama3</td>\n",
       "      <td>0.2501</td>\n",
       "      <td>0.4207</td>\n",
       "      <td>0.8828</td>\n",
       "      <td>0.2143</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>0.4026</td>\n",
       "      <td>0-Shot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Llama3.1</td>\n",
       "      <td>0.2875</td>\n",
       "      <td>0.4470</td>\n",
       "      <td>0.8536</td>\n",
       "      <td>0.1841</td>\n",
       "      <td>0.4376</td>\n",
       "      <td>0.3822</td>\n",
       "      <td>0-Shot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Deepseek-R1</td>\n",
       "      <td>0.2830</td>\n",
       "      <td>0.4420</td>\n",
       "      <td>0.8283</td>\n",
       "      <td>0.1797</td>\n",
       "      <td>0.3754</td>\n",
       "      <td>0.3265</td>\n",
       "      <td>0-Shot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Qwen2.5</td>\n",
       "      <td>0.2535</td>\n",
       "      <td>0.4323</td>\n",
       "      <td>0.8734</td>\n",
       "      <td>0.1845</td>\n",
       "      <td>0.4415</td>\n",
       "      <td>0.4027</td>\n",
       "      <td>0-Shot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Llama3-8B</td>\n",
       "      <td>0.3085</td>\n",
       "      <td>0.3968</td>\n",
       "      <td>0.2532</td>\n",
       "      <td>0.2048</td>\n",
       "      <td>0.3457</td>\n",
       "      <td>0.3365</td>\n",
       "      <td>0-Shot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Prometheus-13b</td>\n",
       "      <td>0.3422</td>\n",
       "      <td>0.4392</td>\n",
       "      <td>0.5494</td>\n",
       "      <td>0.0597</td>\n",
       "      <td>0.1053</td>\n",
       "      <td>0.0962</td>\n",
       "      <td>0-Shot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model     MSE     MAE  Micro QWK  Macro QWK     PCC     SRC  \\\n",
       "0          GPT3.5  0.2331  0.3957     0.2057     0.1271  0.1780  0.1336   \n",
       "1            GPT4  0.3083  0.4517     0.8889     0.2699  0.4958  0.4441   \n",
       "2           GPT4o  0.2539  0.4228     0.1924     0.1431  0.2410  0.2091   \n",
       "3          Llama2  1.2315  0.9558     0.1753     0.0049 -0.0338  0.0024   \n",
       "4          Llama3  0.2501  0.4207     0.8828     0.2143  0.4430  0.4026   \n",
       "5        Llama3.1  0.2875  0.4470     0.8536     0.1841  0.4376  0.3822   \n",
       "6     Deepseek-R1  0.2830  0.4420     0.8283     0.1797  0.3754  0.3265   \n",
       "7         Qwen2.5  0.2535  0.4323     0.8734     0.1845  0.4415  0.4027   \n",
       "8       Llama3-8B  0.3085  0.3968     0.2532     0.2048  0.3457  0.3365   \n",
       "9  Prometheus-13b  0.3422  0.4392     0.5494     0.0597  0.1053  0.0962   \n",
       "\n",
       "  Setting  \n",
       "0  0-Shot  \n",
       "1  0-Shot  \n",
       "2  0-Shot  \n",
       "3  0-Shot  \n",
       "4  0-Shot  \n",
       "5  0-Shot  \n",
       "6  0-Shot  \n",
       "7  0-Shot  \n",
       "8  0-Shot  \n",
       "9  0-Shot  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Per-Prompt QWKs:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Prompt_ID</th>\n",
       "      <th>QWK</th>\n",
       "      <th>Setting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0962</td>\n",
       "      <td>0-Shot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT3.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.1736</td>\n",
       "      <td>0-Shot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GPT3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>0-Shot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GPT3.5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1274</td>\n",
       "      <td>0-Shot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GPT3.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.2815</td>\n",
       "      <td>0-Shot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model  Prompt_ID     QWK Setting\n",
       "0  GPT3.5          1  0.0962  0-Shot\n",
       "1  GPT3.5          2  0.1736  0-Shot\n",
       "2  GPT3.5          3  0.0543  0-Shot\n",
       "3  GPT3.5          4  0.1274  0-Shot\n",
       "4  GPT3.5          5  0.2815  0-Shot"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display Results\n",
    "print(\"Overall Metrics:\")\n",
    "display(results_df)\n",
    "\n",
    "print(\"\\nSample Per-Prompt QWKs:\")\n",
    "display(per_prompt_qwk_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Results DataFrame:\n",
      "            Model     MSE     MAE  Micro QWK  Macro QWK     PCC     SRC  \\\n",
      "0          GPT3.5  0.2331  0.3957     0.2057     0.1271  0.1780  0.1336   \n",
      "1            GPT4  0.3083  0.4517     0.8889     0.2699  0.4958  0.4441   \n",
      "2           GPT4o  0.2539  0.4228     0.1924     0.1431  0.2410  0.2091   \n",
      "3          Llama2  1.2315  0.9558     0.1753     0.0049 -0.0338  0.0024   \n",
      "4          Llama3  0.2501  0.4207     0.8828     0.2143  0.4430  0.4026   \n",
      "5        Llama3.1  0.2875  0.4470     0.8536     0.1841  0.4376  0.3822   \n",
      "6     Deepseek-R1  0.2830  0.4420     0.8283     0.1797  0.3754  0.3265   \n",
      "7         Qwen2.5  0.2535  0.4323     0.8734     0.1845  0.4415  0.4027   \n",
      "8       Llama3-8B  0.3085  0.3968     0.2532     0.2048  0.3457  0.3365   \n",
      "9  Prometheus-13b  0.3422  0.4392     0.5494     0.0597  0.1053  0.0962   \n",
      "\n",
      "  Setting  \n",
      "0  0-Shot  \n",
      "1  0-Shot  \n",
      "2  0-Shot  \n",
      "3  0-Shot  \n",
      "4  0-Shot  \n",
      "5  0-Shot  \n",
      "6  0-Shot  \n",
      "7  0-Shot  \n",
      "8  0-Shot  \n",
      "9  0-Shot  \n",
      "\n",
      "Per-Prompt QWK DataFrame:\n",
      "             Model  Prompt_ID     QWK Setting\n",
      "0           GPT3.5          1  0.0962  0-Shot\n",
      "1           GPT3.5          2  0.1736  0-Shot\n",
      "2           GPT3.5          3  0.0543  0-Shot\n",
      "3           GPT3.5          4  0.1274  0-Shot\n",
      "4           GPT3.5          5  0.2815  0-Shot\n",
      "..             ...        ...     ...     ...\n",
      "75  Prometheus-13b          4  0.0310  0-Shot\n",
      "76  Prometheus-13b          5  0.1419  0-Shot\n",
      "77  Prometheus-13b          6  0.0577  0-Shot\n",
      "78  Prometheus-13b          7  0.0999  0-Shot\n",
      "79  Prometheus-13b          8 -0.0022  0-Shot\n",
      "\n",
      "[80 rows x 4 columns]\n",
      "Results saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "# Display results \n",
    "print(\"Full Results DataFrame:\")\n",
    "print(results_df)\n",
    "\n",
    "print(\"\\nPer-Prompt QWK DataFrame:\")\n",
    "print(per_prompt_qwk_df)\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv(f\"{output_prefix}mQWK.csv\", index=False)\n",
    "per_prompt_qwk_df.to_csv(f\"{output_prefix}-PerPromptQWKs.csv\", index=False)\n",
    "print(\"Results saved to CSV.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarization Quality Evaluation (Few-shot & Zero-shot)\n",
    "This section evaluates the content quality of summaries generated by different LLMs using two common text generation settings:\n",
    "\n",
    "Few-shot: Models were given three carefully selected examples before generating their own summary.\n",
    "\n",
    "Zero-shot: Models generated summaries without seeing any examples.\n",
    "\n",
    "For each model and setting, the evaluation compares the generated summary to a human-written reference summary using:\n",
    "\n",
    "ROUGE-1: Overlap of unigrams (single words)\n",
    "\n",
    "ROUGE-2: Overlap of bigrams (two-word sequences)\n",
    "\n",
    "ROUGE-L: Longest common subsequence\n",
    "\n",
    "METEOR: Alignment-based score that incorporates synonyms and stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping column 'Article' (no 'Few-shot' or 'Zero-shot' in name)\n",
      "\n",
      "Evaluating model: GPT-3.5 Zero-shot  (Zero-shot)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring ROUGE and METEOR: 100%|██████████| 2000/2000 [00:02<00:00, 892.68it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: GPT-4 Zero-shot  (Zero-shot)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring ROUGE and METEOR: 100%|██████████| 2000/2000 [00:04<00:00, 440.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping column 'GPT-4o Zer0-shot ' (no 'Few-shot' or 'Zero-shot' in name)\n",
      "\n",
      "Evaluating model: Llama-3-70B Zero-shot (Zero-shot)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring ROUGE and METEOR: 100%|██████████| 2000/2000 [00:05<00:00, 380.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: Llama-2-70B Zero-shot  (Zero-shot)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring ROUGE and METEOR: 100%|██████████| 2000/2000 [00:05<00:00, 355.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: Llama-3-8B Zero-shot  (Zero-shot)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring ROUGE and METEOR: 100%|██████████| 2000/2000 [00:05<00:00, 386.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: Llama3.1 Zero-shot  (Zero-shot)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring ROUGE and METEOR: 100%|██████████| 2000/2000 [00:05<00:00, 356.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: Qwen2.5-72B Zero-shot  (Zero-shot)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring ROUGE and METEOR: 100%|██████████| 2000/2000 [00:05<00:00, 388.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: Prometheus Zero-shot  (Zero-shot)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring ROUGE and METEOR: 100%|██████████| 2000/2000 [00:05<00:00, 380.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping column 'Unnamed: 11' (no 'Few-shot' or 'Zero-shot' in name)\n",
      "\n",
      "Evaluating model: GPT-3.5 Few-shot  (Few-shot)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring ROUGE and METEOR: 100%|██████████| 2000/2000 [00:04<00:00, 463.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: GPT-4 Few-shot  (Few-shot)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring ROUGE and METEOR: 100%|██████████| 2000/2000 [00:04<00:00, 469.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: GPT-4o Few-shot  (Few-shot)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring ROUGE and METEOR: 100%|██████████| 2000/2000 [00:04<00:00, 439.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: Llama-3-8B Few-shot  (Few-shot)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring ROUGE and METEOR: 100%|██████████| 2000/2000 [00:04<00:00, 408.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: Llama-3-70B Few-shot  (Few-shot)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring ROUGE and METEOR: 100%|██████████| 2000/2000 [00:04<00:00, 409.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: Llama3.1 Few-shot  (Few-shot)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring ROUGE and METEOR: 100%|██████████| 2000/2000 [00:05<00:00, 381.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: Qwen2.5-72B Few-shot  (Few-shot)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring ROUGE and METEOR: 100%|██████████| 2000/2000 [00:04<00:00, 456.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: Llama-2-70B Few-shot  (Few-shot)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring ROUGE and METEOR: 100%|██████████| 2000/2000 [00:04<00:00, 403.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating model: Prometheus Few-shot  (Few-shot)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring ROUGE and METEOR: 100%|██████████| 2000/2000 [00:04<00:00, 435.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Few-shot Evaluation Results:\n",
      "                       ROUGE-1  ROUGE-2  ROUGE-L  METEOR\n",
      "GPT-3.5 Few-shot        0.3605   0.1316   0.2362  0.2718\n",
      "GPT-4 Few-shot          0.3707   0.1457   0.2479  0.2826\n",
      "GPT-4o Few-shot         0.3538   0.1250   0.2270  0.2677\n",
      "Llama-3-8B Few-shot     0.3524   0.1344   0.2308  0.2864\n",
      "Llama-3-70B Few-shot    0.3612   0.1375   0.2347  0.2933\n",
      "Llama3.1 Few-shot       0.2328   0.0642   0.1543  0.1897\n",
      "Qwen2.5-72B Few-shot    0.3629   0.1327   0.2348  0.2685\n",
      "Llama-2-70B Few-shot    0.3422   0.1286   0.2249  0.2778\n",
      "Prometheus Few-shot     0.3454   0.1269   0.2266  0.2692\n",
      "Saved to /Users/koketch/Desktop/summary_eval_results_Few-shot.csv\n",
      "\n",
      "Final Zero-shot Evaluation Results:\n",
      "                        ROUGE-1  ROUGE-2  ROUGE-L  METEOR\n",
      "GPT-3.5 Zero-shot        0.1158   0.0428   0.0775  0.0895\n",
      "GPT-4 Zero-shot          0.3671   0.1453   0.2437  0.2855\n",
      "Llama-3-70B Zero-shot    0.3511   0.1324   0.2245  0.2926\n",
      "Llama-2-70B Zero-shot    0.3339   0.1253   0.2167  0.2855\n",
      "Llama-3-8B Zero-shot     0.3509   0.1330   0.2275  0.2906\n",
      "Llama3.1 Zero-shot       0.3422   0.1289   0.2187  0.2960\n",
      "Qwen2.5-72B Zero-shot    0.3458   0.1236   0.2205  0.2763\n",
      "Prometheus Zero-shot     0.3345   0.1213   0.2170  0.2727\n",
      "Saved to /Users/koketch/Desktop/summary_eval_results_Zero-shot.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "\n",
    "# Optional: download if not already done\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "def compute_metrics(refs, preds):\n",
    "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    metrics = {\n",
    "        \"rouge1\": [],\n",
    "        \"rouge2\": [],\n",
    "        \"rougeL\": [],\n",
    "        \"meteor\": [],\n",
    "    }\n",
    "\n",
    "    for ref, pred in tqdm(zip(refs, preds), total=len(refs), desc=\"Scoring ROUGE and METEOR\"):\n",
    "        r_scores = rouge.score(str(ref), str(pred))\n",
    "        metrics[\"rouge1\"].append(r_scores[\"rouge1\"].fmeasure)\n",
    "        metrics[\"rouge2\"].append(r_scores[\"rouge2\"].fmeasure)\n",
    "        metrics[\"rougeL\"].append(r_scores[\"rougeL\"].fmeasure)\n",
    "        metrics[\"meteor\"].append(meteor_score([str(ref).split()], str(pred).split()))\n",
    "\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "\n",
    "def main(csv_path, output_prefix):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    reference_column = \"Reference Summary\"\n",
    "    if reference_column not in df.columns:\n",
    "        print(\"Missing 'Reference Summary' column.\")\n",
    "        return\n",
    "\n",
    "    # Get all model output columns (exclude the reference)\n",
    "    prediction_columns = [col for col in df.columns if col != reference_column]\n",
    "\n",
    "    # Split into Few-shot and Zero-shot\n",
    "    categorized_results = {\"Few-shot\": {}, \"Zero-shot\": {}}\n",
    "\n",
    "    for col in prediction_columns:\n",
    "        if \"Few-shot\" in col:\n",
    "            setting = \"Few-shot\"\n",
    "        elif \"Zero-shot\" in col:\n",
    "            setting = \"Zero-shot\"\n",
    "        else:\n",
    "            print(f\"Skipping column '{col}' (no 'Few-shot' or 'Zero-shot' in name)\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nEvaluating model: {col} ({setting})\")\n",
    "        ref = df[reference_column].astype(str).tolist()\n",
    "        pred = df[col].astype(str).tolist()\n",
    "        metrics_df = compute_metrics(ref, pred)\n",
    "        categorized_results[setting][col] = metrics_df.mean()\n",
    "\n",
    "    for setting, model_dict in categorized_results.items():\n",
    "        if not model_dict:\n",
    "            print(f\"No {setting} columns found.\")\n",
    "            continue\n",
    "\n",
    "        final_results = pd.DataFrame(model_dict).T\n",
    "        final_results = final_results[[\"rouge1\", \"rouge2\", \"rougeL\", \"meteor\"]]\n",
    "        final_results.columns = [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"METEOR\"]\n",
    "        final_results = final_results.round(4)\n",
    "\n",
    "        print(f\"\\nFinal {setting} Evaluation Results:\")\n",
    "        print(final_results)\n",
    "\n",
    "        save_path = f\"{output_prefix}_{setting}.csv\"\n",
    "        final_results.to_csv(save_path)\n",
    "        print(f\"Saved to {save_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = \"/Users/koketch/Desktop/Summarizations.csv\"\n",
    "    output_prefix = \"/Users/koketch/Desktop/summary_eval_results\"\n",
    "\n",
    "    main(csv_path, output_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
